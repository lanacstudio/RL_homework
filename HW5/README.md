# HW4 Deep reinforcement learning in action code (chap3) using

1.  è©¦è·‘ä¸€ä¸‹ HW_Template=dqn_...OK(å·²ä¿®æ­£).ipynb Code, è®“ä»–æ­£å¸¸run è¨“ç·´å®Œæˆ
2.  use chatgpt æ”¹å¯«chap 3.8 ç¬¬ä¸‰ç«  to Pytorch lightning
3. åŠ ä¸Šcall back (Tensorboard, early stop, dump best model, etc)

**ä½œæ¥­è¦æ±‚**ï¼š 

    (1) è©¦è·‘ä¸€ä¸‹ HW_Template=dqn_...OK(å·²ä¿®æ­£).ipynb Code, è®“ä»–æ­£å¸¸run è¨“ç·´å®Œæˆ
    
    (2) use chatgpt æ”¹å¯«chap 3.8 ç¬¬ä¸‰ç«  to Pytorch lightning

    (3) åŠ ä¸Šcall back (Tensorboard, early stop, dump best model, etc)
      
**âœ… (1) çš„å®Œæ•´ç‰ˆ code ï¼† è·‘å®Œçš„çµæœéƒ½åœ¨ ğŸ—‚ï¸HW45/HW5 DQN PyTorch.ipynb è£¡**
**âœ… (2)ã€(3) çš„å®Œæ•´ç‰ˆ code ï¼† è·‘å®Œçš„çµæœéƒ½åœ¨ ğŸ—‚ï¸HW45/HW5_38_pytorch.ipynb è£¡**



## 1. è©¦è·‘ä¸€ä¸‹ HW_Template=dqn_...OK(å·²ä¿®æ­£).ipynb Code, è®“ä»–æ­£å¸¸run è¨“ç·´å®Œæˆ

**(1) ipnyb æª”è£¡çš„ç‰‡æ®µæˆªåœ– ï¼ˆå®Œæ•´ code åœ¨ ğŸ—‚ï¸HW45/HW5 DQN PyTorch.ipynbï¼‰**

<img src="https://github.com/lanacstudio/RL_homework/blob/main/HW5/imgs/screenshot1.png" width="60%"/>

<img src="https://github.com/lanacstudio/RL_homework/blob/main/HW5/imgs/screenshot2.png" width="60%"/>

<img src="https://github.com/lanacstudio/RL_homework/blob/main/HW5/imgs/screenshot3.png" width="60%"/>

<img src="https://github.com/lanacstudio/RL_homework/blob/main/HW5/imgs/screenshot4.png" width="60%"/>


**(2) é™„ä¸Šéƒ¨åˆ†code with è¨»è§£**


```python

# å¼•å…¥å¿…è¦çš„å‡½å¼åº«
import copy
import torch
import torch.nn.functional as F
from torch.optim import AdamW
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset
from pytorch_lightning import LightningModule

# å®šç¾© DeepQLearning é¡åˆ¥ï¼Œç¹¼æ‰¿è‡ª LightningModule
class DeepQLearning(LightningModule):

  # åˆå§‹åŒ–æ–¹æ³•
  def __init__(self, env_name, policy=epsilon_greedy, capacity=100_000, batch_size=256, lr=1e-3,
               hidden_size=128, gamma=0.99, loss_fn=F.smooth_l1_loss, optim=AdamW,
               eps_start=1.0, eps_end=0.15, eps_last_episode=100, samples_per_epoch=10_000, sync_rate=10):

    super().__init__()

    # å‰µå»ºç’°å¢ƒ
    self.env = create_environment(env_name)
    obs_size = self.env.observation_space.shape[0]  # è§€å¯Ÿç©ºé–“å¤§å°
    n_actions = self.env.action_space.n  # è¡Œå‹•ç©ºé–“å¤§å°

    # å‰µå»º Q ç¶²è·¯å’Œç›®æ¨™ Q ç¶²è·¯
    self.q_net = DQN(hidden_size, obs_size, n_actions)
    self.target_q_net = copy.deepcopy(self.q_net)

    self.policy = policy  # ä½¿ç”¨çš„ç­–ç•¥
    self.buffer = ReplayBuffer(capacity=capacity)  # ç¶“é©—å›æ”¾ç·©è¡å€
    self.save_hyperparameters()  # å„²å­˜è¶…åƒæ•¸

    # åœ¨ç·©è¡å€ä¸­è£œæ»¿æŒ‡å®šæ•¸é‡çš„æ¨£æœ¬
    while len(self.buffer) < self.hparams.samples_per_epoch:
      print(f"{len(self.buffer)} å€‹æ¨£æœ¬åœ¨ç¶“é©—ç·©è¡å€ä¸­ã€‚æ­£åœ¨è£œæ»¿...")
      self.play_episode(epsilon=self.hparams.eps_start)

  # ç„¡æ¢¯åº¦è¨ˆç®—çš„æ–¹æ³•ï¼Œæ¨¡æ“¬ä¸€å€‹å›åˆçš„éŠç©éç¨‹
  @torch.no_grad()
  def play_episode(self, policy=None, epsilon=0.):
    state = self.env.reset()  # é‡ç½®ç’°å¢ƒç‹€æ…‹
    done = False

    while not done:
      if policy:
        action = policy(state, self.env, self.q_net, epsilon=epsilon)  # æ ¹æ“šç­–ç•¥é¸æ“‡å‹•ä½œ
      else:
        action = self.env.action_space.sample()  # éš¨æ©Ÿé¸æ“‡å‹•ä½œ

      next_state, reward, done, info = self.env.step(action)  # åŸ·è¡Œå‹•ä½œä¸¦è§€å¯Ÿä¸‹ä¸€å€‹ç‹€æ…‹å’Œçå‹µ
      exp = (state, action, reward, done, next_state)  # å‰µå»ºä¸€å€‹ç¶“é©—å…ƒçµ„
      self.buffer.append(exp)  # å°‡ç¶“é©—å…ƒçµ„åŠ å…¥ç¶“é©—ç·©è¡å€
      state = next_state  # æ›´æ–°ç•¶å‰ç‹€æ…‹ç‚ºä¸‹ä¸€å€‹ç‹€æ…‹

  # å‰å‘å‚³æ’­æ–¹æ³•
  def forward(self, x):
    return self.q_net(x)

  # é…ç½®å„ªåŒ–å™¨
  def configure_optimizers(self):
    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)
    return [q_net_optimizer]

  # è¨“ç·´æ•¸æ“šåŠ è¼‰å™¨
  def train_dataloader(self):
    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)
    dataloader = DataLoader(dataset=dataset, batch_size=self.hparams.batch_size)
    return dataloader

  # è¨“ç·´æ­¥é©Ÿ
  def training_step(self, batch, batch_idx):
    states, actions, rewards, dones, next_states = batch
    actions = actions.unsqueeze(1)
    rewards = rewards.unsqueeze(1)
    dones = dones.unsqueeze(1)

    state_action_values = self.q_net(states).gather(1, actions)  # è¨ˆç®—ç‹€æ…‹å‹•ä½œå€¼
    next_action_values, _ = self.target_q_net(next_states).max(dim=1, keepdim=True)  # è¨ˆç®—ç›®æ¨™ Q ç¶²è·¯çš„æœ€å¤§å‹•ä½œå€¼

    next_action_values[dones] = 0.0  # å°‡çµ‚æ­¢ç‹€æ…‹çš„ç›®æ¨™ Q å€¼è¨­ç‚º 0

    expected_state_action_values = rewards + self.hparams.gamma * next_action_values  # è¨ˆç®—é æœŸç‹€æ…‹å‹•ä½œå€¼
    loss = self.hparams.loss_fn(state_action_values, expected_state_action_values)  # è¨ˆç®—æå¤±å€¼
    self.log('episode/Q-Error', loss)  # è¨˜éŒ„æå¤±å€¼åˆ°æ—¥èªŒ
    return loss

  # æ¯å€‹è¨“ç·´å‘¨æœŸçµæŸå¾ŒåŸ·è¡Œçš„æ–¹æ³•
  def training_epoch_end(self, training_step_outputs):
    epsilon = max(
      self.hparams.eps_end,
      self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode
    )
    self.play_episode(policy=self.policy, epsilon=epsilon)  # ä½¿ç”¨æ›´æ–°çš„ç­–ç•¥é€²è¡Œä¸€æ¬¡éŠç©
    self.log('episode/Return', self.env.return_queue[-1])  # è¨˜éŒ„å›åˆçš„å›å ±å€¼åˆ°æ—¥èªŒ

    if self.current_epoch % self.hparams.sync_rate == 0:
      self.target_q_net.load_state_dict(self.q_net.state_dict())  # æ›´æ–°ç›®æ¨™ Q ç¶²è·¯çš„åƒæ•¸

# å®šç¾© ReplayBuffer é¡åˆ¥ï¼Œç”¨æ–¼å­˜å„²å’Œå–æ¨£ç¶“é©—
class ReplayBuffer:
  def __init__(self, capacity):
    self.capacity = capacity
    self.buffer = []

  def __len__(self):
    return len(self.buffer)

  def append(self, experience):
    self.buffer.append(experience)
    if len(self.buffer) > self.capacity:
      self.buffer.pop(0)

  def sample(self, batch_size):
    indices = np.random.choice(len(self.buffer), batch_size, replace=False)
    batch = [self.buffer[idx] for idx in indices]
    return zip(*batch)

# å®šç¾© RLDataset é¡åˆ¥ï¼Œç”¨æ–¼ç”Ÿæˆè¨“ç·´æ•¸æ“šé›†
class RLDataset(Dataset):
  def __init__(self, buffer, samples_per_epoch):
    self.buffer = buffer
    self.samples_per_epoch = samples_per_epoch

  def __len__(self):
    return self.samples_per_epoch

  def __getitem__(self, idx):
    return self.buffer[idx]

# å®šç¾© DQN é¡åˆ¥ï¼Œç”¨æ–¼è¡¨ç¤ºæ·±åº¦ Q ç¶²è·¯
class DQN(torch.nn.Module):
  def __init__(self, hidden_size, obs_size, n_actions):
    super(DQN, self).__init__()
    self.fc1 = torch.nn.Linear(obs_size, hidden_size)
    self.fc2 = torch.nn.Linear(hidden_size, n_actions)

  def forward(self, x):
    x = F.relu(self.fc1(x))
    x = self.fc2(x)
    return x

# å®šç¾© epsilon_greedy ç­–ç•¥å‡½æ•¸
def epsilon_greedy(state, env, q_net, epsilon=0.1):
  if torch.rand(1) < epsilon:
    return env.action_space.sample()
  else:
    with torch.no_grad():
      q_values = q_net(torch.tensor(state, dtype=torch.float32))
      return torch.argmax(q_values).item()

# å‰µå»ºç’°å¢ƒçš„å‡½æ•¸ï¼ˆé€™è£¡åƒ…ç‚ºç¤ºä¾‹ï¼Œå¯¦éš›ä½¿ç”¨æ™‚éœ€æ›¿æ›ç‚ºé©åˆçš„ç’°å¢ƒå‰µå»ºæ–¹æ³•ï¼‰
def create_environment(env_name):
  return gym.make(env_name)


```




## 2. use chatgpt æ”¹å¯«chap 3.8 ç¬¬ä¸‰ç«  to Pytorch lightning
    
**âœ… çš„å®Œæ•´ç‰ˆ code ï¼† è·‘å®Œçš„çµæœéƒ½åœ¨ ğŸ—‚ï¸HW5/HW5_38_pytorch.ipynb è£¡ (åªç•™3.8çš„code)**
**ğŸ”† å¯«äº†å…©ç‰ˆæ”¹å¯«ï¼Œåˆ†åˆ¥ç‚º `code(1)` è·Ÿ `code(2)`**


### `code(1)` with è¨»è§£**


**(1) è¼‰å…¥ pytorch ç­‰æ‰€éœ€å¥—ä»¶**


```python
import matplotlib.pyplot as plt
import numpy as np
import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
import torch
import torch.nn as nn
import torch.nn.functional as F

```

**(2) å®šç¾©å‹•ä½œï¼†class**


```python
# å‡è¨­å‹•ä½œé›†åˆå·²å®šç¾©
action_set = ['ä¸Š', 'ä¸‹', 'å·¦', 'å³']

# å®šç¾© Q ç¶²è·¯æ¶æ§‹
class QNet(pl.LightningModule):
    def __init__(self, input_dim=64, output_dim=4):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 128)  # ç¬¬ä¸€å±¤å…¨é€£æ¥å±¤
        self.fc2 = nn.Linear(128, 64)         # ç¬¬äºŒå±¤å…¨é€£æ¥å±¤
        self.fc3 = nn.Linear(64, output_dim)  # è¼¸å‡ºå±¤

    def forward(self, x):
        x = F.relu(self.fc1(x))  # ReLU æ¿€æ´»å‡½æ•¸
        x = F.relu(self.fc2(x))  # ReLU æ¿€æ´»å‡½æ•¸
        return self.fc3(x)       # è¿”å›æœ€çµ‚è¼¸å‡º

    def training_step(self, batch, batch_idx):
        # å®šç¾©è¨“ç·´æ­¥é©Ÿï¼Œé€™è£¡å¯ä»¥æ ¹æ“šéœ€è¦ä¿®æ”¹
        x, y = batch
        y_hat = self.forward(x)
        loss = F.cross_entropy(y_hat, y)
        self.log('train_loss', loss)  # è¨˜éŒ„è¨“ç·´æå¤±
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters())
```


**(3) åˆå§‹åŒ– LightningModule å’Œ Trainer**
 
```python
gridworld_model = QNet()
early_stopping = EarlyStopping(monitor='train_loss', patience=5, mode='min')
gridworld_trainer = pl.Trainer(max_epochs=5000, gpus=1, callbacks=[early_stopping])
```


**(4) é–‹å§‹è¨“ç·´**
 
```python
# è™›æ“¬çš„è¨“ç·´å¾ªç’°ä¾†ç”Ÿæˆæå¤±
losses = []
for epoch in range(1, 5001):
    # æ¨¡æ“¬ç”Ÿæˆç¬¦åˆæ¨¡å‹è¦æ±‚çš„éš¨æ©Ÿè¼¸å…¥å’Œæ¨™ç±¤
    x = torch.randn(1, 64)  # å‡è¨­è¼¸å…¥ç¶­åº¦ç‚º (1, 64)
    y = torch.randint(0, 4, (1,))  # å‡è¨­æ¨™ç±¤ç¶­åº¦ç‚º (1,)

    # ä½¿ç”¨ trainer çš„ training_step æ–¹æ³•é€²è¡Œè¨“ç·´æ­¥é©Ÿ
    output = gridworld_model.training_step((x, y), epoch)
    
    # æª¢æŸ¥æ—©åœæ¢ä»¶
    if gridworld_trainer.should_stop:
        print("Early stopping criteria met")
        break

    # æ¨¡æ“¬å°å‡ºæº–ç¢ºç‡ç­‰è³‡è¨Š
    if epoch % 100 == 0:
        print(f"Epoch [{epoch}/5000], Loss: {output:.4f}")

    # å°‡æå¤±è¨˜éŒ„ä¸‹ä¾†
    losses.append(output.item())
```



**(5) ç¹ªåœ–**
 
```python
# ç¹ªè£½è¨“ç·´æå¤±æ›²ç·š
plt.figure(figsize=(10, 6))
plt.plot(losses, label='Train Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Variation"')
plt.legend()
plt.grid(True)
plt.show()
```


**(6) æˆªåœ–**

<img src="https://github.com/lanacstudio/RL_homework/blob/main/HW5/imgs/code1.png" width="60%"/>


### `code(2)` with è¨»è§£ (only é‡è¦çš„ç¨‹å¼ç¢¼å€å¡Šï¼Œå®Œæ•´ç‰ˆè¦çœ‹ .ipynb)**



**(1) Q ç¶²è·¯æ¨¡å‹å®šç¾© (QNet class)**

```python
class QNet(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(QNet, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)  # ç¬¬ä¸€å±¤å…¨é€£æ¥å±¤
        self.fc2 = nn.Linear(128, 64)         # ç¬¬äºŒå±¤å…¨é€£æ¥å±¤
        self.fc3 = nn.Linear(64, output_dim)  # è¼¸å‡ºå±¤

    def forward(self, x):
        x = F.relu(self.fc1(x))  # ReLU æ¿€æ´»å‡½æ•¸
        x = F.relu(self.fc2(x))  # ReLU æ¿€æ´»å‡½æ•¸
        return self.fc3(x)       # è¿”å›æœ€çµ‚è¼¸å‡º

```

**(2) LightningModule å®šç¾© (LightningGridworld class)**

```python
class LightningGridworld(pl.LightningModule):
    def __init__(self, size=4, mem_size=1000, batch_size=200, sync_freq=500, gamma=0.9, max_moves=50, epsilon_decay=1/5000):
        super(LightningGridworld, self).__init__()
        self.size = size
        self.mem_size = mem_size
        self.batch_size = batch_size
        self.sync_freq = sync_freq
        self.gamma = gamma
        self.max_moves = max_moves
        self.epsilon_decay = epsilon_decay

        # å®šç¾©æ¨¡å‹
        self.model = QNet(64, 4)
        self.model2 = QNet(64, 4).eval()  # ç›®æ¨™ç¶²è·¯

        # å®šç¾©å„ªåŒ–å™¨å’Œæå¤±å‡½æ•¸
        self.optimizer = optim.Adam(self.model.parameters())
        self.loss_fn = nn.MSELoss()

        # åˆå§‹åŒ–å›æ”¾è¨˜æ†¶
        self.replay = deque(maxlen=self.mem_size)

        # å…¶ä»–è®Šæ•¸
        self.epsilon = 1.0

    def forward(self, x):
        return self.model(x)

```

**(3) è¨“ç·´æ­¥é©Ÿ (training_step method)**

```python
    def training_step(self, batch, batch_idx):
        state1_batch, action_batch, reward_batch, state2_batch, done_batch = batch

        Q1 = self.model(state1_batch)
        with torch.no_grad():
            Q2 = self.model2(state2_batch)

        Y = reward_batch + self.gamma * ((1 - done_batch) * torch.max(Q2, dim=1)[0])
        X = Q1.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()
        loss = self.loss_fn(X, Y.detach())

        self.log('train_loss', loss)  # ä½¿ç”¨ self.log è¨˜éŒ„è¨“ç·´æå¤±

        return loss

```

**(4) é©—è­‰æ­¥é©Ÿ (validation_step method)**

```python
    def validation_step(self, batch, batch_idx):
        state1_batch, action_batch, reward_batch, state2_batch, done_batch = batch

        Q1 = self.model(state1_batch)
        with torch.no_grad():
            Q2 = self.model2(state2_batch)

        Y = reward_batch + self.gamma * ((1 - done_batch) * torch.max(Q2, dim=1)[0])
        X = Q1.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()
        loss = self.loss_fn(X, Y.detach())

        self.log('val_loss', loss)  # ä½¿ç”¨ self.log è¨˜éŒ„é©—è­‰æå¤±

```

**(5) è¨­ç½®æ—©åœæ©Ÿåˆ¶ (EarlyStopping callback)**

```python
# åˆå§‹åŒ– LightningGridworld å’Œ Trainer
gridworld_model = LightningGridworld()
early_stop_callback = pl.callbacks.EarlyStopping(monitor='avg_val_loss', patience=5, verbose=True, mode='min')  # å®šç¾© early stop æ©Ÿåˆ¶
gridworld_trainer = pl.Trainer(max_epochs=5000, gpus=1, callbacks=[early_stop_callback])  # å¦‚æœæœ‰GPUå‰‡å•Ÿç”¨GPUè¨“ç·´

```

**(6) è¨“ç·´æ¨¡å‹ (fit æ–¹æ³•)**

```python
# è¨“ç·´æ¨¡å‹
gridworld_trainer.fit(gridworld_model)

```


**(7) æˆªåœ–**

<img src="https://github.com/lanacstudio/RL_homework/blob/main/HW5/imgs/code2.png" width="60%"/>




## (3) åŠ ä¸Šcall back (Tensorboard, early stop, dump best model, etc)

åŠ å…¥ early stop çš„ code ç‰‡æ®µï¼š

**`code (1)`:**
 
```python
gridworld_model = QNet()
early_stopping = EarlyStopping(monitor='train_loss', patience=5, mode='min')
gridworld_trainer = pl.Trainer(max_epochs=5000, gpus=1, callbacks=[early_stopping])
```

**`code (2)`:**

```python
gridworld_model = LightningGridworld()
early_stop_callback = pl.callbacks.EarlyStopping(monitor='avg_val_loss', patience=5, verbose=True, mode='min')  # å®šç¾© early stop æ©Ÿåˆ¶
gridworld_trainer = pl.Trainer(max_epochs=5000, gpus=1, callbacks=[early_stop_callback])  # å¦‚æœæœ‰GPUå‰‡å•Ÿç”¨GPUè¨“ç·´
```

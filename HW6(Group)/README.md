# HW6 Advance DRL in final project you use (æ‰¾Github or yourown)


# OUTLINE

    ğŸ”· 0. å°ˆæ¡ˆ code
    ğŸ”· 1. æœŸæœ«å°ˆæ¡ˆç°¡ä»‹
    ğŸ”· 2. Q-Learning åŠ Îµ-greedy ä»‹ç´¹ & æœ¬å°ˆæ¡ˆä½¿ç”¨çš„ RL code
    ğŸ”· 3. ç›¸é—œ Github åˆ†äº«

## ğŸ”· 0. å°ˆæ¡ˆ code

### æœŸæœ«å°ˆæ¡ˆ NLPower Assisantï¼š

1. å°ˆæ¡ˆ GitHub é€£çµï¼š [NLPowerAsistant](https://github.com/lanac0911/NLPowerAsistant)

2. å¼·åŒ–å¼å­¸ç¿’çš„ codeï¼š [NLPowerAsistant/test/anomaly detection.py](https://github.com/lanac0911/NLPowerAsistant/tree/main/test)

## ğŸ”· 1. æœŸæœ«å°ˆæ¡ˆç°¡ä»‹

### 1ï¸âƒ£. ä»‹ç´¹

   é€šéçµåˆ NILM èˆ‡ NLP çš„æŠ€è¡“å¯¦ç¾ä¸€å€‹ user-friendly çš„æºé€šä»‹é¢ï¼Œè©²ä»‹é¢å¯ä»¥å¯¦ç¾æ™ºæ…§å®¶å±…çš„æ“ä½œã€‚

  ![Q-learning](https://github.com/lanacstudio/RL_homework/blob/main/HW6(Group)/imgs/nlp.png)


 ### 2ï¸âƒ£. é¡å¤–åŠŸèƒ½

  * çµåˆäº† **å¼·åŒ–å¼å­¸ç¿’** çš„æŠ€è¡“åš **ç•°å¸¸åµæ¸¬ï¼ˆAnomaly Detectionï¼‰**
  * åŠ å…¥ ã€Œç¯€ç´„æé†’ã€ã€ã€Œç•°å¸¸å ±å‘Šã€åŠŸèƒ½

 ![Q-learning](https://github.com/lanacstudio/RL_homework/blob/main/HW6(Group)/imgs/screen.png)


 ### 3ï¸âƒ£. Flow Chart

  * çµåˆäº† **å¼·åŒ–å¼å­¸ç¿’** çš„æŠ€è¡“åš **ç•°å¸¸åµæ¸¬ï¼ˆAnomaly Detectionï¼‰**
  * åŠ å…¥ ã€Œç¯€ç´„æé†’ã€ã€ã€Œç•°å¸¸å ±å‘Šã€åŠŸèƒ½

 ![Q-learning](https://github.com/lanacstudio/RL_homework/blob/main/HW6(Group)/imgs/flow.png)


 ### 4ï¸âƒ£. ä»‹é¢æˆªåœ–

 ![Q-learning](https://github.com/lanacstudio/RL_homework/blob/main/HW6(Group)/imgs/screen2.png)

## ğŸ”· 2. Q-Learning åŠ Îµ-greedy ä»‹ç´¹ & æœ¬å°ˆæ¡ˆä½¿ç”¨çš„ RL code

### â¬›ï¸ Q-Learning

### &emsp; 1ï¸âƒ£. ä»‹ç´¹


Q-learningæ˜¯ä¸€ç¨®ç„¡æ¨¡å‹ï¼ˆmodel-freeï¼‰çš„å¼·åŒ–å­¸ç¿’ç®—æ³•ï¼Œç”¨æ–¼æ‰¾å‡ºçµ¦å®šç’°å¢ƒä¸­çš„æœ€å„ªå‹•ä½œç­–ç•¥ã€‚

æ˜¯é€šéå­¸ç¿’ç‹€æ…‹-è¡Œå‹•å€¼å‡½æ•¸ 
$Q(s,a)$ ä¾†æ‰¾å‡ºæœ€å„ªç­–ç•¥ã€‚åœ¨æ¯å€‹æ™‚é–“æ­¥ï¼Œæ™ºèƒ½é«”è§€å¯Ÿç•¶å‰ç‹€æ…‹ $s$ï¼Œé¸æ“‡ä¸¦åŸ·è¡Œè¡Œå‹• $a$ï¼Œç„¶å¾Œæ ¹æ“šç²å¾—çš„çå‹µ $r$ å’Œåˆ°é”çš„æ–°ç‹€æ…‹ $sâ€²$ æ›´æ–° $Q$ å€¼ã€‚æœ€çµ‚ï¼Œæ™ºèƒ½é«”å­¸æœƒåœ¨æ¯å€‹ç‹€æ…‹ä¸‹é¸æ“‡èƒ½æœ€å¤§åŒ–ç´¯ç©çå‹µçš„è¡Œå‹•ã€‚

![Q-learning](https://github.com/lanacstudio/RL_homework/blob/main/HW6(Group)/imgs/Q-learning.png)

æ›´æ–°å…¬å¼ï¼š

![Q-func](https://github.com/lanacstudio/RL_homework/blob/main/HW6(Group)/imgs/q-func.png)

å…¶ä¸­ï¼š

* ç•¶å‰ $Q$ å€¼ï¼š $Q(s,a)$
* å­¸ç¿’ç‡ Î±ï¼šæ±ºå®šäº†æ–°ä¿¡æ¯åœ¨æ›´æ–°éç¨‹ä¸­çš„æ¬Šé‡ï¼Œç¯„åœæ˜¯ 0 åˆ° 1 ä¹‹é–“ã€‚é«˜å­¸ç¿’ç‡æ„å‘³è‘—æ›´ä¾è³´æ–°ä¿¡æ¯ï¼Œä½å­¸ç¿’ç‡æ„å‘³è‘—æ›´ä¾è³´å·²æœ‰ Q å€¼ã€‚
* å³æ™‚çå‹µ $r$ï¼šæ™ºèƒ½é«”åœ¨ç•¶å‰ç‹€æ…‹ $s$ åŸ·è¡Œè¡Œå‹• $a$ å¾Œç²å¾—çš„çå‹µã€‚
* æŠ˜æ‰£å› å­ Î³ï¼šç¯„åœæ˜¯ 0 åˆ° 1 ä¹‹é–“ï¼Œç”¨æ–¼å¹³è¡¡å³æ™‚çå‹µå’Œæœªä¾†çå‹µã€‚é«˜æŠ˜æ‰£å› å­æ„å‘³è‘—æ›´é‡è¦–æœªä¾†çå‹µï¼Œä½æŠ˜æ‰£å› å­å‰‡æ›´é‡è¦–å³æ™‚çå‹µã€‚
* æœ€å¤§æœªä¾† Q å€¼ $max a' Q(s',a')$ åœ¨ä¸‹ä¸€ç‹€æ…‹ $s'$ ä¸‹çš„æ‰€æœ‰å¯èƒ½è¡Œå‹•ä¸­é¸æ“‡æœ€å¤§çš„ $Q$ å€¼ã€‚

### &emsp; 2ï¸âƒ£. å°ˆæ¡ˆçš„ RL code


```python
class Agent:
    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):
        self.q_table = np.zeros((num_states, num_actions))  # åˆå§‹åŒ–Qè¡¨ç‚ºé›¶
        self.learning_rate = learning_rate  # å­¸ç¿’ç‡
        self.discount_factor = discount_factor  # æŠ˜æ‰£å› å­
        self.exploration_rate = exploration_rate  # æ¢ç´¢ç‡

    # Q-learningçš„éƒ¨åˆ†ï¼šæ›´æ–°Qå€¼
    def update_q_table(self, state, action, reward, next_state):
        if next_state < self.q_table.shape[0]:  # ç¢ºä¿ç´¢å¼•åœ¨é‚Šç•Œä¹‹å…§
            predict = self.q_table[state, action]  # ç•¶å‰Qå€¼
            target = reward + self.discount_factor * np.max(self.q_table[next_state, :])  # ç›®æ¨™Qå€¼
            self.q_table[state, action] += self.learning_rate * (target - predict)  # æ›´æ–°Qå€¼

```

predictæ˜¯ç•¶å‰çš„ $Q$ å€¼ï¼Œtargetæ˜¯åŸºæ–¼çå‹µå’Œä¸‹ä¸€å€‹ç‹€æ…‹çš„æœ€å¤§ $Q$ å€¼è¨ˆç®—å‡ºçš„ç›®æ¨™å€¼ã€‚ $Q$ å€¼é€šéå­¸ç¿’ç‡æ›´æ–°å…¬å¼é€²è¡Œæ›´æ–°

### â¬›ï¸ Îµ-greedy 

### &emsp; 1ï¸âƒ£. ä»‹ç´¹

Îµ-greedy ç­–ç•¥ç”¨æ–¼æ±ºå®šæ™ºèƒ½é«”åœ¨æ¯å€‹ç‹€æ…‹ä¸‹çš„è¡Œå‹•é¸æ“‡ï¼Œä»¥å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ã€‚æ¢ç´¢æ˜¯æŒ‡å˜—è©¦æ–°çš„æˆ–ä¸å¸¸ç”¨çš„è¡Œå‹•ï¼Œä»¥ç™¼ç¾æ›´å¤šå¯èƒ½çš„é«˜çå‹µè¡Œå‹•ã€‚åˆ©ç”¨æ˜¯æŒ‡é¸æ“‡ç•¶å‰å·²çŸ¥çš„æœ€ä½³è¡Œå‹•ï¼Œä»¥ç²å¾—æœ€å¤§å³æ™‚çå‹µã€‚

![Q-learning](https://github.com/lanacstudio/RL_homework/blob/main/HW6(Group)/imgs/greedy.png)


### &emsp; 2ï¸âƒ£. å°ˆæ¡ˆçš„ RL code


```python
    # Îµ-greedyç­–ç•¥çš„éƒ¨åˆ†ï¼šé¸æ“‡å‹•ä½œ
    def choose_action(self, state):
        if np.random.uniform(0, 1) < self.exploration_rate:  # Îµçš„æ¦‚ç‡ä¸‹é¸æ“‡éš¨æ©Ÿå‹•ä½œï¼ˆæ¢ç´¢ï¼‰
            return np.random.choice(self.q_table.shape[1])
        else:  # 1-Îµçš„æ¦‚ç‡ä¸‹é¸æ“‡Qå€¼æœ€å¤§çš„å‹•ä½œï¼ˆåˆ©ç”¨ï¼‰
            return np.argmax(self.q_table[state, :])
```



### â¬›ï¸ ç¶œåˆ Q-Learning å’Œ Îµ-greedy 

```python
# å®šç¾©æ™ºèƒ½é«”é¡
class Agent:
    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):
        self.q_table = np.zeros((num_states, num_actions))  # åˆå§‹åŒ–Qè¡¨ç‚ºé›¶
        self.learning_rate = learning_rate  # å­¸ç¿’ç‡
        self.discount_factor = discount_factor  # æŠ˜æ‰£å› å­
        self.exploration_rate = exploration_rate  # æ¢ç´¢ç‡

    # Îµ-greedyç­–ç•¥çš„éƒ¨åˆ†ï¼šé¸æ“‡å‹•ä½œ
    def choose_action(self, state):
        if np.random.uniform(0, 1) < self.exploration_rate:  # Îµçš„æ¦‚ç‡ä¸‹é¸æ“‡éš¨æ©Ÿå‹•ä½œï¼ˆæ¢ç´¢ï¼‰
            return np.random.choice(self.q_table.shape[1])
        else:  # 1-Îµçš„æ¦‚ç‡ä¸‹é¸æ“‡Qå€¼æœ€å¤§çš„å‹•ä½œï¼ˆåˆ©ç”¨ï¼‰
            return np.argmax(self.q_table[state, :])

    # Q-learningçš„éƒ¨åˆ†ï¼šæ›´æ–°Qå€¼
    def update_q_table(self, state, action, reward, next_state):
        if next_state < self.q_table.shape[0]:  # ç¢ºä¿ç´¢å¼•åœ¨é‚Šç•Œä¹‹å…§
            predict = self.q_table[state, action]  # ç•¶å‰Qå€¼
            target = reward + self.discount_factor * np.max(self.q_table[next_state, :])  # ç›®æ¨™Qå€¼
            self.q_table[state, action] += self.learning_rate * (target - predict)  # æ›´æ–°Qå€¼

```

    
    
## ğŸ”· 3. ç›¸é—œ Github åˆ†äº«

1ï¸âƒ£. [ç¶²å€](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow)

![Q-learning](https://github.com/lanacstudio/RL_homework/blob/main/HW6(Group)/imgs/git1.png)

2ï¸âƒ£. [ç¶²å€](https://github.com/vmayoral/basic_reinforcement_learning)

![Q-learning](https://github.com/lanacstudio/RL_homework/blob/main/HW6(Group)/imgs/git2.png)

3ï¸âƒ£. [ç¶²å€](https://github.com/sudharsan13296/Hands-On-Reinforcement-Learning-With-Python)

![Q-learning](https://github.com/lanacstudio/RL_homework/blob/main/HW6(Group)/imgs/git3.png)

